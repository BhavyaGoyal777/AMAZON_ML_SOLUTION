{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_DIR = \"DATA/student_resource/dataset\"\n",
    "    TEXT_MODEL = \"roberta-large\"\n",
    "    MAX_EPOCHS = 15\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MAX_LENGTH = 512\n",
    "    VAL_SPLIT = 0.15\n",
    "    RANDOM_STATE = 42\n",
    "    NUM_WORKERS = 4\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "    DROPOUT = 0.3\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    DEVICE = device\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(config.DATA_DIR, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(config.DATA_DIR, 'test.csv'))\n",
    "train_df['log_price'] = np.log1p(train_df['price'])\n",
    "\n",
    "train_data, val_data = train_test_split(train_df, test_size=config.VAL_SPLIT, random_state=config.RANDOM_STATE)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, config, is_test=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            row['catalog_content'],\n",
    "            max_length=self.config.MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            item['label'] = torch.FloatTensor([row['log_price']])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class RoBERTaFFN(nn.Module):\n    def __init__(self, model_name, dropout=0.3, finetuned_path=None, load_encoder_weights=None):\n        super().__init__()\n        # Load base model architecture\n        print(f\"Loading base model architecture: {model_name}\")\n        self.encoder = AutoModel.from_pretrained(model_name)\n\n        # If fine-tuned encoder weights are provided, load them\n        if load_encoder_weights:\n            print(f\"Loading fine-tuned encoder weights from: {load_encoder_weights}\")\n            checkpoint = torch.load(load_encoder_weights, map_location='cpu')\n            \n            # Handle different checkpoint formats\n            if isinstance(checkpoint, dict):\n                if 'encoder' in checkpoint:\n                    self.encoder.load_state_dict(checkpoint['encoder'])\n                elif 'model_state_dict' in checkpoint:\n                    # If full model was saved, extract encoder weights\n                    encoder_state = {k.replace('encoder.', ''): v for k, v in checkpoint['model_state_dict'].items() if k.startswith('encoder.')}\n                    self.encoder.load_state_dict(encoder_state)\n                elif 'state_dict' in checkpoint:\n                    encoder_state = {k.replace('encoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('encoder.')}\n                    self.encoder.load_state_dict(encoder_state)\n                else:\n                    # Assume it's the encoder state dict directly\n                    self.encoder.load_state_dict(checkpoint)\n            else:\n                self.encoder.load_state_dict(checkpoint)\n            \n            print(\"âœ“ Fine-tuned weights loaded successfully!\")\n        \n        # If finetuned_path is a directory (HF format), load from there\n        elif finetuned_path:\n            print(f\"Loading fine-tuned model from directory: {finetuned_path}\")\n            self.encoder = AutoModel.from_pretrained(finetuned_path)\n\n        # Freeze all except FFN\n        for name, param in self.encoder.named_parameters():\n            if 'intermediate' in name or 'output.dense' in name:\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n        \n        hidden_size = self.encoder.config.hidden_size\n        \n        # 3-layer head\n        self.regressor = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 1)\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]\n        return self.regressor(cls_embedding).squeeze(-1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.TEXT_MODEL)\n",
    "\n",
    "train_dataset = TextDataset(train_data, tokenizer, config)\n",
    "val_dataset = TextDataset(val_data, tokenizer, config)\n",
    "test_dataset = TextDataset(test_df, tokenizer, config, is_test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE*2, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE*2, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_loader)} | Val: {len(val_loader)} | Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# OPTION 1: Load fine-tuned weights from .pth file\n# ============================================================\nfinetuned_weights_path = \"./your_finetuned_roberta.pth\"  # <-- UPDATE THIS PATH\nmodel = RoBERTaFFN(\n    config.TEXT_MODEL, \n    dropout=config.DROPOUT, \n    load_encoder_weights=finetuned_weights_path\n).to(device)\n\n# ============================================================\n# OPTION 2: Load from Hugging Face directory\n# ============================================================\n# finetuned_path = \"./path/to/finetuned-roberta-dir\"\n# model = RoBERTaFFN(config.TEXT_MODEL, dropout=config.DROPOUT, finetuned_path=finetuned_path).to(device)\n\n# ============================================================\n# OPTION 3: Use base model (no fine-tuning)\n# ============================================================\n# model = RoBERTaFFN(config.TEXT_MODEL, dropout=config.DROPOUT).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total: {total_params:,} | Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.MAX_EPOCHS, eta_min=1e-6)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "def calculate_smape(predictions, actuals):\n",
    "    predictions = np.expm1(predictions)\n",
    "    actuals = np.expm1(actuals)\n",
    "    numerator = np.abs(predictions - actuals)\n",
    "    denominator = (np.abs(actuals) + np.abs(predictions)) / 2\n",
    "    return np.mean(numerator / np.maximum(denominator, 1e-8)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, scaler, config):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device).squeeze()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(input_ids, attention_mask)\n",
    "                loss = criterion(preds, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            preds = model(input_ids, attention_mask)\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device).squeeze()\n",
    "        \n",
    "        preds = model(input_ids, attention_mask)\n",
    "        loss = criterion(preds, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    smape = calculate_smape(np.array(all_preds), np.array(all_labels))\n",
    "    \n",
    "    return avg_loss, smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smape = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(config.MAX_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.MAX_EPOCHS} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, config)\n",
    "    val_loss, val_smape = validate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train: {train_loss:.4f} | Val: {val_loss:.4f} | SMAPE: {val_smape:.2f}%\")\n",
    "    \n",
    "    if val_smape < best_smape:\n",
    "        best_smape = val_smape\n",
    "        torch.save(model.state_dict(), 'best_roberta_ffn.pth')\n",
    "        print(f\"âœ“ Best: {val_smape:.2f}%\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nâœ“ Best SMAPE: {best_smape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_roberta_ffn.pth', map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        preds = model(input_ids, attention_mask)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "test_prices = np.expm1(np.array(all_preds))\n",
    "submission = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': test_prices})\n",
    "submission.to_csv('submission_roberta_ffn.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Saved: submission_roberta_ffn.csv\")\n",
    "print(f\"âœ“ SMAPE: {best_smape:.2f}%\")\n",
    "print(f\"âœ“ Range: ${submission['price'].min():.2f} - ${submission['price'].max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}