{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_DIR = \"DATA/student_resource/dataset\"\n",
    "    TRAIN_IMAGE_EMB = \"train_image_emb.npy\"  # (N, 1280)\n",
    "    TEST_IMAGE_EMB = \"test_image_emb.npy\"    # (75000, 1280)\n",
    "    \n",
    "    TEXT_MODEL = \"roberta-large\"\n",
    "    MAX_EPOCHS = 15\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MAX_LENGTH = 512\n",
    "    VAL_SPLIT = 0.15\n",
    "    RANDOM_STATE = 42\n",
    "    NUM_WORKERS = 4\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "    DROPOUT = 0.3\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    DEVICE = device\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(config.DATA_DIR, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(config.DATA_DIR, 'test.csv'))\n",
    "train_df['log_price'] = np.log1p(train_df['price'])\n",
    "\n",
    "train_data, val_data = train_test_split(train_df, test_size=config.VAL_SPLIT, random_state=config.RANDOM_STATE)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image embeddings\n",
    "full_train_image_emb = np.load(config.TRAIN_IMAGE_EMB)\n",
    "test_image_emb = np.load(config.TEST_IMAGE_EMB)\n",
    "\n",
    "print(f\"Train image emb: {full_train_image_emb.shape}\")\n",
    "print(f\"Test image emb: {test_image_emb.shape}\")\n",
    "\n",
    "# Split image embeddings\n",
    "train_idx = train_data.index.values\n",
    "val_idx = val_data.index.values\n",
    "\n",
    "train_image_emb = full_train_image_emb[train_idx]\n",
    "val_image_emb = full_train_image_emb[val_idx]\n",
    "\n",
    "print(f\"Train image split: {train_image_emb.shape}\")\n",
    "print(f\"Val image split: {val_image_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, df, image_emb, tokenizer, config, is_test=False):\n",
    "        self.df = df\n",
    "        self.image_emb = image_emb\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            row['catalog_content'],\n",
    "            max_length=self.config.MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'image_emb': torch.from_numpy(self.image_emb[idx]).float()\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            item['label'] = torch.FloatTensor([row['log_price']])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaImageConcat(nn.Module):\n",
    "    def __init__(self, model_name, image_dim=1280, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze all except FFN\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if 'intermediate' in name or 'output.dense' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        text_dim = self.encoder.config.hidden_size  # 1024\n",
    "        \n",
    "        # 3-layer head: concat(1024, 1280) = 2304\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(text_dim + image_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image_emb):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_hidden = outputs.last_hidden_state[:, 0, :]\n",
    "        fused = torch.cat([text_hidden, image_emb], dim=-1)\n",
    "        return self.regressor(fused).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.TEXT_MODEL)\n",
    "\n",
    "train_dataset = FusionDataset(train_data, train_image_emb, tokenizer, config)\n",
    "val_dataset = FusionDataset(val_data, val_image_emb, tokenizer, config)\n",
    "test_dataset = FusionDataset(test_df, test_image_emb, tokenizer, config, is_test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE*2, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE*2, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_loader)} | Val: {len(val_loader)} | Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RoBERTaImageConcat(config.TEXT_MODEL, image_dim=1280, dropout=config.DROPOUT).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total: {total_params:,} | Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.MAX_EPOCHS, eta_min=1e-6)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "def calculate_smape(predictions, actuals):\n",
    "    predictions = np.expm1(predictions)\n",
    "    actuals = np.expm1(actuals)\n",
    "    numerator = np.abs(predictions - actuals)\n",
    "    denominator = (np.abs(actuals) + np.abs(predictions)) / 2\n",
    "    return np.mean(numerator / np.maximum(denominator, 1e-8)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, scaler, config):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        image_emb = batch['image_emb'].to(device)\n",
    "        labels = batch['label'].to(device).squeeze()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(input_ids, attention_mask, image_emb)\n",
    "                loss = criterion(preds, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            preds = model(input_ids, attention_mask, image_emb)\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        image_emb = batch['image_emb'].to(device)\n",
    "        labels = batch['label'].to(device).squeeze()\n",
    "        \n",
    "        preds = model(input_ids, attention_mask, image_emb)\n",
    "        loss = criterion(preds, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    smape = calculate_smape(np.array(all_preds), np.array(all_labels))\n",
    "    \n",
    "    return avg_loss, smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smape = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(config.MAX_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.MAX_EPOCHS} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, config)\n",
    "    val_loss, val_smape = validate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train: {train_loss:.4f} | Val: {val_loss:.4f} | SMAPE: {val_smape:.2f}%\")\n",
    "    \n",
    "    if val_smape < best_smape:\n",
    "        best_smape = val_smape\n",
    "        torch.save(model.state_dict(), 'best_roberta_image_concat.pth')\n",
    "        print(f\"✓ Best: {val_smape:.2f}%\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n✓ Best SMAPE: {best_smape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_roberta_image_concat.pth', map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        image_emb = batch['image_emb'].to(device)\n",
    "        preds = model(input_ids, attention_mask, image_emb)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "test_prices = np.expm1(np.array(all_preds))\n",
    "submission = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': test_prices})\n",
    "submission.to_csv('submission_roberta_image_concat.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved: submission_roberta_image_concat.csv\")\n",
    "print(f\"✓ SMAPE: {best_smape:.2f}%\")\n",
    "print(f\"✓ Range: ${submission['price'].min():.2f} - ${submission['price'].max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
